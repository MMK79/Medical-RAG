# Fine-Tune Medical BERT on MEDMCQA Dataset
## Environment Variables
Hold values of the current environment, like operating system, user session and etc.
### Path
`PATH` give a hint to the OS, for places to look for executable programs

> [!tip] .env file contain individual user variables that override the variables that are set in /etc/environment file

> [!question] How to use `.env` file?
> Solution:
> `from dotenv import load_dotenv`
> `load_dotenv()`
> [Link](https://dev.to/jakewitcher/using-env-files-for-environment-variables-in-python-applications-55a1)

> [!tip] `os.environ` will return a dictionary of all your environment variables, you can even define your own environment variable with use of `os.environ`

> [!danger] Code Error
> * Error:
>    ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`
> * Solution:
> `! pip install -U accelerate`
> `! pip install -U transformers`
>   Update the package and restart the session

> [!warning] Code Warning:
> Problem: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector
> Solution: on Single GPU code will work fine but on Multiple GPU you need to use DDP instead of DP [Link](https://docs.pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel)
## Preprocess MEDMCQA
> [!tip] Morphy Functions
> a set of morphology functions that aim to convert a given string into its base or dictionary form $\xrightarrow{used\ in}$ `nltk.stem.WordNetLemmentizer.lemmetize`

> [!tips]  `model.train()` vs. `model.eval()` & `torch.no_grad()` in PyTorch
> `model.train()` is default mode of PyTorch $\rightarrow$ some layer work differently, like; Dropout, 
> * Dropout: randomly zeros some input of tensor $\xrightarrow{why?}$ to lower chance of model getting overfit
>  * BatchNorm: Normalize data base on statistics (mean + variance) of current batch
>  
> `model.eval` when you are done with training and want to validate/test your data $\xrightarrow{cause}$ disable Dropout and change the behavior of BatchNorm (now it will use the learned running statistics instead of the batch specific statistics)
> 
> Avoid unnecessary memory usage + Improve performance $\rightarrow$ Combo `model.eval()` with `torch.no_grad()` (disable gradient calculation, ensure gradient won't get calculated for the operation inside the block)
> `model.eval()`
> `with torch.no_grad():`
> `out_data = model(data)`

> [!tip] `.to(device)` usage
> It is necessary to have "model" and your "data" on the same device $\xrightarrow{else}$ it will cause "Runtime Error"

> [!tip] Attention Mask
> It will get generated by the tokenizer itself
> BERT use Attention Mechanism to focus on relevant part of data
> Binary Tensor (0, 1)
> 0 $\xrightarrow{means}$ Padding, Ignore it (won't impact the hidden states(model knowledge/understanding) + output)
> 1 $\xrightarrow{means}$ Real Data, Pay Attention (use it in attention computation)