{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 13:15:01.133391: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753794901.331840      89 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753794901.388188      89 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open /users/masoudkord/downloads/wordnet_source.zip, /users/masoudkord/downloads/wordnet_source.zip.zip or /users/masoudkord/downloads/wordnet_source.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForMaskedLM,\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import HfApi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "!unzip -qn /users/masoudkord/downloads/wordnet_source.zip -d  /users/masoudkord/downloads/wordnet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15095MB, multi_processor_count=40, uuid=42f5d29f-e760-4a70-68ea-6dfc70dcbd4d, L2_cache_size=4MB)\n",
      "_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15095MB, multi_processor_count=40, uuid=16e938e0-1c7b-2ec0-09ff-cc2851fb4d68, L2_cache_size=4MB)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() == 0:\n",
    "    print(\"No GPU Available!\")\n",
    "else:\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_properties(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\"platform\": \"Linux\", \"platform-release\": \"6.6.56+\", \"platform-version\": \"#1 '\n",
      " 'SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\", \"architecture\": [\"64bit\", '\n",
      " '\"ELF\"], \"hostname\": \"20677a442f23\", \"ip-address\": \"172.19.2.2\", '\n",
      " '\"mac-address\": \"02:42:ac:13:02:02\", \"processor\": \"x86_64\", \"ram\": \"31 GB\"}')\n"
     ]
    }
   ],
   "source": [
    "import platform, socket, re, uuid, json, psutil, logging\n",
    "\n",
    "def getSystemInfo():\n",
    "    try:\n",
    "        info = {}\n",
    "        info[\"platform\"]=platform.system()\n",
    "        info[\"platform-release\"]=platform.release()\n",
    "        info[\"platform-version\"]=platform.version()\n",
    "        info[\"architecture\"]=platform.architecture()\n",
    "        info[\"architecture\"]=platform.architecture()\n",
    "        info[\"hostname\"]=socket.gethostname()\n",
    "        info[\"ip-address\"]=socket.gethostbyname(socket.gethostname())\n",
    "        info[\"mac-address\"]=\":\".join(re.findall('..', '%012x' % uuid.getnode()))\n",
    "        info[\"processor\"]=platform.processor()\n",
    "        info[\"ram\"]=str(round(psutil.virtual_memory().total / (1024.0**3))) + \" GB\"\n",
    "        return json.dumps(info)\n",
    "    except Exception as e:\n",
    "        logging.exception(e)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(getSystemInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "trainset_range = list(range(18000, 38000))\n",
    "medmcqa_dataset_path = \"openlifescienceai/medmcqa\"\n",
    "base_bert_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "finetuned_bert_path = (\n",
    "    \"BioClinicalBert-MLM-Finetuned-40k-25epoch-exp-25epoch-questions.pth\"\n",
    ")\n",
    "dataset_file_name = (\n",
    "    \"MEDMCQA-dataset-with-CLS-40k-25epoch-exp-25epoch-questions-nltk.json\"\n",
    ")\n",
    "repo_id = \"MMK79/Medical-RAG\"\n",
    "# push_dataset_to_huggingface = True # After the First run disable it\n",
    "push_dataset_to_huggingface = False\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MEDMCQA Dataset and Select Columns\n",
    "'exp' which is a explanation about why the answer is the right answer <br>\n",
    "we ignore the columns that have low explanation ('exp') <br>\n",
    "we ignore the columns that don't have explanation ('exp') <br>\n",
    "we ignore the columns that don't have questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c777faeff347278e76a421d273f9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf193bcdd6b4258863136c6601bbc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/85.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f607f2303f874c6e9bc4e212cdd2bc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/936k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c0328e0d9d44b6b5c6c345a0ef8c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7bba0a738247148eb9789373e4b1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/182822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f24adc51ea4d3ea33e59a52758f831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/6150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ca57cb519948518a0f404755749551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4183 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea27f5aca83f479a8933357ad93d19a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 16831\n"
     ]
    }
   ],
   "source": [
    "def filter_none(example):\n",
    "    return (\n",
    "        (example[\"exp\"] is not None)\n",
    "        and (len(example[\"exp\"]) > 20)\n",
    "        and (example[\"question\"] is not None)\n",
    "    )\n",
    "\n",
    "\n",
    "dataset = load_dataset(medmcqa_dataset_path)\n",
    "dataset = dataset[\"train\"].select(trainset_range)\n",
    "dataset = dataset.filter(filter_none).select_columns([\"question\", \"exp\"])\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "dataset_df = pd.DataFrame(dataset.to_dict())\n",
    "print(f\"dataset length: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Fine-tuned Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5f7ccb8f7e4162bbc005ccf13aa47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f49cf5ec4444f59a8c308da37360fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9640b17fda412eb661d82419c26e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4259722f4fa24ea58a2887bc6f0e7ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3732dd0dc278458cb4b08d6a931af43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ed-40k-25epoch-exp-25epoch-questions.pth:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(base_bert_path)\n",
    "model = BertForMaskedLM.from_pretrained(base_bert_path).to(device)\n",
    "\n",
    "checkpoint_file = hf_hub_download(repo_id=repo_id, filename=finetuned_bert_path)\n",
    "checkpoint = torch.load(checkpoint_file)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model = model.bert  # dropping MLM head\n",
    "model.eval() # to test/inference your model --> it is a switch\n",
    "# model.train() # to go in training mode again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # list comprehension --> new_list = [expression for iterable if condition]\n",
    "    # isalpha() --> check if all the characters in word are letter --> return a boolean \n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    # delete stop works\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isalpha test\n",
    "s = \"What%\"\n",
    "s.isalpha()\n",
    "# WordNetLemmatizer() test\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "# Lemmatize using WordNet’s built-in morphy function. Returns the input word unchanged if it cannot be found in WordNet.\n",
    "# morphy functions: a set of morphology functions that aim to convert a given string into its base or dictionary form\n",
    "wl.lemmatize('its')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76f34b05daf41338688af790ef9e5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test tqdm --> show progress bar\n",
    "for i in tqdm(range(1000)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7f47ba603042389bf8d3bd3a5b9ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/526 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 128\n",
    "# Classification tokens --> a special, learnable token that is added to the beginning of every input sequence\n",
    "cls_tokens = []\n",
    "for batch in tqdm(dataloader):\n",
    "    batch[\"question\"] = [preprocess_text(txt) for txt in batch[\"question\"]]\n",
    "    # Tokenization\n",
    "    tokens = tokenizer(\n",
    "        batch[\"question\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # need to use to(device) to have the model and data on the same device --> else you get runtime error\n",
    "    input_ids = tokens[\"input_ids\"].to(device)\n",
    "    # it will get generated automatically by the tokenizer itself\n",
    "    # attention mask is a binary tensor (a list of 0, 1) --> tell which tokens are real data and which tokens are padding --> 1=real data(pay attention), 0=padding (ignore)\n",
    "    # We need it cause models like bert use Attention Mechanism to focus on relevant parts of the input --> without attention mask, model will try to understand paddings too\n",
    "    # So real word get used in attention computation\n",
    "    # Padding will get ignore and won't affect hidden states (model understanding/knowledge) and output\n",
    "    att_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # forward pass --> takeing input data, propagating through all the layers and producing output\n",
    "        outputs = model(input_ids, att_mask)\n",
    "\n",
    "    # CLS Extraction\n",
    "    # only available if you use the full model --> BERT\n",
    "    # Transform CLS through linear + tanh layer\n",
    "    # shape=(batch_size, hidden_dim)\n",
    "    if \"pooler_output\" in outputs:\n",
    "        cls_embedding = outputs.pooler_output\n",
    "    elif \"last_hidden_state\" in outputs:\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    else:\n",
    "        raise Exception(\"No CLS token found in the given model\")\n",
    "\n",
    "    # Storing the embedding\n",
    "    cls_embedding = cls_embedding.cpu().numpy().tolist()\n",
    "    cls_tokens += cls_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "last_hidden_state\n",
      "torch.Size([31, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "len(outputs)\n",
    "dir(type(outputs))\n",
    "outputs.items()\n",
    "for k, v in outputs.items():\n",
    "    # print(k, v)\n",
    "    pass\n",
    "print(outputs.hidden_states)\n",
    "print(outputs.attentions)\n",
    "# print(outputs.last_hidden_state)\n",
    "for k in outputs.keys():\n",
    "    print(k)\n",
    "# dir(type(outputs.last_hidden_state)) # shape is available\n",
    "# output of each token in the last layer\n",
    "print(outputs.last_hidden_state.shape) # batchsize, seq_len, hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>exp</th>\n",
       "      <th>question_cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All of the following are pyrogenic cytokines, ...</td>\n",
       "      <td>Interleukin 18 is not a pyrogenic cytokine. IL...</td>\n",
       "      <td>[0.4302745461463928, -0.4610498547554016, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40-year old female presented with neck swellin...</td>\n",
       "      <td>Ref. Robbins Pathology. 9th edition. Page. 109...</td>\n",
       "      <td>[1.1794304847717285, 0.1141890361905098, -0.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Following statement regarding dislocation of t...</td>\n",
       "      <td>Anterior dislocation is more common in which h...</td>\n",
       "      <td>[0.024791469797492027, -0.19909602403640747, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The active search for unrecognized disease or ...</td>\n",
       "      <td>Screening is the search for unrecognized disea...</td>\n",
       "      <td>[0.6333976984024048, 0.0940687358379364, -0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fir tree pattern lesion is seen in</td>\n",
       "      <td>Fir tree pattern of distribution of lesions is...</td>\n",
       "      <td>[1.1372100114822388, 0.17038805782794952, -0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16826</th>\n",
       "      <td>Carcinoma sigmoid colon with obstruction Manag...</td>\n",
       "      <td>- Obstruction due to rectosigmoid growth with ...</td>\n",
       "      <td>[0.6879968047142029, 0.10079113394021988, -0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16827</th>\n",
       "      <td>ADHD in childhood can lead to which of the fol...</td>\n",
       "      <td>ADHD can lead to substance abuse,mood disorder...</td>\n",
       "      <td>[0.37708356976509094, 0.5247248411178589, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16828</th>\n",
       "      <td>Nerve for adductor compament of thigh ?</td>\n",
       "      <td>Ans. B) Obturator nerveObturator nerve is the ...</td>\n",
       "      <td>[1.0885707139968872, -0.436038076877594, -0.68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16829</th>\n",
       "      <td>The &amp;;a&amp;;wave of jugular venous pulse is produ...</td>\n",
       "      <td>JVP or jugular venous is a reflection of the r...</td>\n",
       "      <td>[0.23046493530273438, -0.13350751996040344, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16830</th>\n",
       "      <td>Antibody diversity is due to-</td>\n",
       "      <td>Hypervariable region - The amino acid sequence...</td>\n",
       "      <td>[1.0974578857421875, -0.6757969856262207, -0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16831 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "0      All of the following are pyrogenic cytokines, ...   \n",
       "1      40-year old female presented with neck swellin...   \n",
       "2      Following statement regarding dislocation of t...   \n",
       "3      The active search for unrecognized disease or ...   \n",
       "4                     Fir tree pattern lesion is seen in   \n",
       "...                                                  ...   \n",
       "16826  Carcinoma sigmoid colon with obstruction Manag...   \n",
       "16827  ADHD in childhood can lead to which of the fol...   \n",
       "16828            Nerve for adductor compament of thigh ?   \n",
       "16829  The &;a&;wave of jugular venous pulse is produ...   \n",
       "16830                      Antibody diversity is due to-   \n",
       "\n",
       "                                                     exp  \\\n",
       "0      Interleukin 18 is not a pyrogenic cytokine. IL...   \n",
       "1      Ref. Robbins Pathology. 9th edition. Page. 109...   \n",
       "2      Anterior dislocation is more common in which h...   \n",
       "3      Screening is the search for unrecognized disea...   \n",
       "4      Fir tree pattern of distribution of lesions is...   \n",
       "...                                                  ...   \n",
       "16826  - Obstruction due to rectosigmoid growth with ...   \n",
       "16827  ADHD can lead to substance abuse,mood disorder...   \n",
       "16828  Ans. B) Obturator nerveObturator nerve is the ...   \n",
       "16829  JVP or jugular venous is a reflection of the r...   \n",
       "16830  Hypervariable region - The amino acid sequence...   \n",
       "\n",
       "                                            question_cls  \n",
       "0      [0.4302745461463928, -0.4610498547554016, -0.1...  \n",
       "1      [1.1794304847717285, 0.1141890361905098, -0.11...  \n",
       "2      [0.024791469797492027, -0.19909602403640747, -...  \n",
       "3      [0.6333976984024048, 0.0940687358379364, -0.01...  \n",
       "4      [1.1372100114822388, 0.17038805782794952, -0.2...  \n",
       "...                                                  ...  \n",
       "16826  [0.6879968047142029, 0.10079113394021988, -0.6...  \n",
       "16827  [0.37708356976509094, 0.5247248411178589, -0.1...  \n",
       "16828  [1.0885707139968872, -0.436038076877594, -0.68...  \n",
       "16829  [0.23046493530273438, -0.13350751996040344, 0....  \n",
       "16830  [1.0974578857421875, -0.6757969856262207, -0.2...  \n",
       "\n",
       "[16831 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting cls tokens to np.array\n",
    "cls_tokens = [np.asarray(cls) for cls in cls_tokens]\n",
    "dataset_df[\"question_cls\"] = cls_tokens\n",
    "print(len(dataset_df))\n",
    "print(type(dataset_df))\n",
    "print(display(dataset_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of decimal places to use when encoding floating point values.\n",
    "# Maximum is 15 --> more than 15 == Value Error\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html\n",
    "dataset_df.to_json(dataset_file_name, double_precision=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HuggingFace_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821dcd9fd7a14aca9019d8dfa1cdfce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MEDMCQA-dataset-with-CLS-40k-25epoch-exp-25epoch-questions-nltk.json:   0%|          | 0.00/249M [00:00<?, ?B/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if push_dataset_to_huggingface:\n",
    "    # generate a token from Profile > Setting > Access Tokens with write access\n",
    "    api = HfApi(\n",
    "        token=secret_value_0,\n",
    "    )\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=f\"./{dataset_file_name}\",\n",
    "        path_in_repo=dataset_file_name,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
